# -*- coding: utf-8 -*-
"""compare_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cFXG1xxujfxJO-pMq0LD4PfwVtCs_KkP
"""

"""
Model Comparison Script
=======================
Compare CodeBERT results with baseline models (RF, SVM, MLP)

Author: Shouvik Banik
Date: December 2025
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

def load_baseline_results(dataset_name):
    """Load baseline model results from Week 1"""
    baseline_file = f'results/{dataset_name.upper()}_baseline_results.csv'

    if os.path.exists(baseline_file):
        df = pd.read_csv(baseline_file)
        return df
    else:
        print(f"âš ï¸ Baseline results not found: {baseline_file}")
        return None

def load_codebert_results(dataset_name):
    """Load CodeBERT results from Week 2"""
    codebert_file = f'results/{dataset_name}_codebert_results.csv'

    if os.path.exists(codebert_file):
        df = pd.read_csv(codebert_file)
        return df
    else:
        print(f"âš ï¸ CodeBERT results not found: {codebert_file}")
        return None

def create_comparison_table(dataset_name):
    """Create comparison table for all models"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š COMPARISON TABLE - {dataset_name.upper()}")
    print(f"{'='*80}\n")

    # Load results
    baseline = load_baseline_results(dataset_name)
    codebert = load_codebert_results(dataset_name)

    if baseline is None or codebert is None:
        print(f"âŒ Cannot create comparison - missing data")
        return None

    # Combine results
    all_results = pd.concat([baseline, codebert], ignore_index=True)

    # Print table
    print(all_results.to_string(index=False))
    print(f"\n{'='*80}")

    return all_results

def plot_comparison(dataset_name, all_results):
    """Create comparison visualizations"""
    print(f"\nğŸ“Š Creating comparison charts for {dataset_name.upper()}...")

    # Prepare data
    models = all_results['Model'].tolist()
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']

    # Convert string percentages to float
    for metric in metrics:
        all_results[metric] = all_results[metric].astype(float)

    # Create figure
    fig, axes = plt.subplots(2, 3, figsize=(16, 10))
    fig.suptitle(f'Model Comparison - {dataset_name.upper()}\nCodeBERT vs Baseline Models',
                 fontsize=16, fontweight='bold')

    colors = ['#2ecc71', '#3498db', '#e74c3c', '#9b59b6']  # RF, SVM, MLP, CodeBERT

    # Plot each metric
    for idx, metric in enumerate(metrics):
        row = idx // 3
        col = idx % 3
        ax = axes[row, col]

        values = all_results[metric].tolist()
        bars = ax.bar(range(len(models)), values, color=colors[:len(models)])

        ax.set_title(metric, fontsize=13, fontweight='bold')
        ax.set_ylabel('Score', fontsize=11)
        ax.set_xticks(range(len(models)))
        ax.set_xticklabels(models, rotation=0, fontsize=10)
        ax.set_ylim([0, 1.0])
        ax.grid(axis='y', alpha=0.3)

        # Add value labels
        for i, (bar, val) in enumerate(zip(bars, values)):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                   f'{val:.3f}',
                   ha='center', va='bottom', fontsize=9, fontweight='bold')

            # Highlight CodeBERT if best
            if i == len(models) - 1 and val == max(values):
                bar.set_edgecolor('gold')
                bar.set_linewidth(3)

    # Hide last subplot
    axes[1, 2].axis('off')

    # Add improvement statistics in the empty subplot
    ax = axes[1, 2]
    ax.text(0.1, 0.9, 'CodeBERT Improvements:', fontsize=12, fontweight='bold',
            transform=ax.transAxes)

    y_pos = 0.75
    for metric in metrics:
        baseline_best = all_results[metric].iloc[:-1].max()  # Best baseline
        codebert_val = all_results[metric].iloc[-1]  # CodeBERT
        improvement = ((codebert_val - baseline_best) / baseline_best) * 100

        symbol = 'â†‘' if improvement > 0 else 'â†“'
        color = 'green' if improvement > 0 else 'red'

        ax.text(0.1, y_pos, f"{metric}: {symbol} {abs(improvement):.1f}%",
               fontsize=10, color=color, transform=ax.transAxes)
        y_pos -= 0.12

    plt.tight_layout()

    # Save figure
    output_file = f'results/{dataset_name}_model_comparison.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"âœ… Saved: {output_file}")

    plt.close()

def create_combined_comparison(datasets=['cm1', 'pc1', 'jm1', 'kc1']):
    """Create combined comparison across all datasets"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š CREATING COMBINED COMPARISON CHART")
    print(f"{'='*80}\n")

    all_data = []

    for dataset in datasets:
        baseline = load_baseline_results(dataset)
        codebert = load_codebert_results(dataset)

        if baseline is not None and codebert is not None:
            # Get best baseline AUC-ROC
            baseline['AUC-ROC'] = baseline['AUC-ROC'].astype(float)
            best_baseline_auc = baseline['AUC-ROC'].max()
            best_baseline_model = baseline.loc[baseline['AUC-ROC'].idxmax(), 'Model']

            # Get CodeBERT AUC-ROC
            codebert['AUC-ROC'] = codebert['AUC-ROC'].astype(float)
            codebert_auc = codebert['AUC-ROC'].iloc[0]

            all_data.append({
                'Dataset': dataset.upper(),
                'Best Baseline': best_baseline_model,
                'Baseline AUC': best_baseline_auc,
                'CodeBERT AUC': codebert_auc,
                'Improvement': codebert_auc - best_baseline_auc
            })

    if not all_data:
        print("âŒ No data available for combined comparison")
        return

    df = pd.DataFrame(all_data)

    # Print table
    print("\nğŸ“Š COMBINED RESULTS TABLE:")
    print(f"{'='*80}")
    print(df.to_string(index=False))
    print(f"{'='*80}")

    # Calculate statistics
    avg_improvement = df['Improvement'].mean()
    print(f"\nğŸ“ˆ AVERAGE IMPROVEMENT: {avg_improvement:+.4f} AUC-ROC")
    print(f"   CodeBERT wins: {(df['Improvement'] > 0).sum()}/{len(df)} datasets")

    # Create visualization
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    fig.suptitle('CodeBERT vs Best Baseline - All Datasets',
                 fontsize=14, fontweight='bold')

    # Plot 1: AUC-ROC comparison
    x = np.arange(len(df))
    width = 0.35

    ax1.bar(x - width/2, df['Baseline AUC'], width, label='Best Baseline',
            color='#3498db', alpha=0.8)
    ax1.bar(x + width/2, df['CodeBERT AUC'], width, label='CodeBERT',
            color='#9b59b6', alpha=0.8)

    ax1.set_xlabel('Dataset', fontsize=11)
    ax1.set_ylabel('AUC-ROC Score', fontsize=11)
    ax1.set_title('AUC-ROC Comparison', fontsize=12, fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels(df['Dataset'])
    ax1.legend()
    ax1.grid(axis='y', alpha=0.3)
    ax1.set_ylim([0.5, 1.0])

    # Add value labels
    for i, (baseline, codebert) in enumerate(zip(df['Baseline AUC'], df['CodeBERT AUC'])):
        ax1.text(i - width/2, baseline + 0.02, f'{baseline:.3f}',
                ha='center', va='bottom', fontsize=9)
        ax1.text(i + width/2, codebert + 0.02, f'{codebert:.3f}',
                ha='center', va='bottom', fontsize=9, fontweight='bold')

    # Plot 2: Improvement bars
    colors = ['green' if x > 0 else 'red' for x in df['Improvement']]
    ax2.bar(df['Dataset'], df['Improvement'], color=colors, alpha=0.7)
    ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.8)

    ax2.set_xlabel('Dataset', fontsize=11)
    ax2.set_ylabel('AUC-ROC Improvement', fontsize=11)
    ax2.set_title('CodeBERT Improvement Over Best Baseline', fontsize=12, fontweight='bold')
    ax2.grid(axis='y', alpha=0.3)

    # Add value labels
    for i, (dataset, improvement) in enumerate(zip(df['Dataset'], df['Improvement'])):
        va = 'bottom' if improvement > 0 else 'top'
        y_offset = 0.005 if improvement > 0 else -0.005
        ax2.text(i, improvement + y_offset, f'{improvement:+.3f}',
                ha='center', va=va, fontsize=10, fontweight='bold')

    plt.tight_layout()

    # Save
    output_file = 'results/combined_model_comparison.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"\nâœ… Saved: {output_file}")

    plt.close()

    return df

def main():
    """Main comparison function"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘        MODEL COMPARISON: CODEBERT VS BASELINES               â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    datasets = ['cm1', 'pc1', 'jm1', 'kc1']

    # Individual comparisons
    for dataset in datasets:
        results = create_comparison_table(dataset)
        if results is not None:
            plot_comparison(dataset, results)

    # Combined comparison
    combined_df = create_combined_comparison(datasets)

    print(f"\n{'='*80}")
    print(f"âœ… ALL COMPARISONS COMPLETE!")
    print(f"ğŸ“ Check results/ folder for comparison charts")
    print(f"{'='*80}\n")

if __name__ == "__main__":
    main()