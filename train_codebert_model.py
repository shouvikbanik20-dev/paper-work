# -*- coding: utf-8 -*-
"""train_codebert_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cGUMIe6GUkOg5l9Yjhpdm0GjqSHl15Vz
"""

"""
CodeBERT Fine-Tuning for Software Defect Prediction
====================================================
This script fine-tunes CodeBERT transformer model for binary defect classification.

Author: Shouvik Banik
Date: December 2025
"""

import os
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    RobertaTokenizer,
    RobertaForSequenceClassification,
    AdamW,
    get_linear_schedule_with_warmup
)
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, confusion_matrix,
    classification_report
)
from tqdm import tqdm
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)

class DefectDataset(Dataset):
    """Dataset class for defect prediction with CodeBERT"""

    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = int(self.labels[idx])

        # Tokenize
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

class CodeBERTTrainer:
    """Trainer class for CodeBERT defect prediction"""

    def __init__(self, dataset_name='cm1', device=None):
        self.dataset_name = dataset_name
        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.tokenizer = None
        self.train_losses = []
        self.val_metrics = []

        print(f"\n{'='*80}")
        print(f"ğŸš€ INITIALIZING CODEBERT TRAINER")
        print(f"{'='*80}")
        print(f"Dataset: {dataset_name.upper()}")
        print(f"Device: {self.device.upper()}")
        print(f"{'='*80}\n")

    def load_data(self):
        """Load training and test data"""
        print("ğŸ“‚ Loading datasets...")

        train_file = f'data/codebert/{self.dataset_name}_train_code.csv'
        test_file = f'data/codebert/{self.dataset_name}_test_code.csv'

        train_df = pd.read_csv(train_file)
        test_df = pd.read_csv(test_file)

        self.train_texts = train_df['code'].tolist()
        self.train_labels = train_df['label'].tolist()
        self.test_texts = test_df['code'].tolist()
        self.test_labels = test_df['label'].tolist()

        print(f"âœ… Data loaded!")
        print(f"   Train: {len(self.train_texts)} samples")
        print(f"   Test:  {len(self.test_texts)} samples")
        print(f"   Train defects: {sum(self.train_labels)} ({sum(self.train_labels)/len(self.train_labels)*100:.1f}%)")

    def initialize_model(self):
        """Initialize CodeBERT model and tokenizer"""
        print("\nğŸ¤– Initializing CodeBERT model...")

        # Load tokenizer (RoBERTa-based)
        self.tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')

        # Load model with classification head
        self.model = RobertaForSequenceClassification.from_pretrained(
            'microsoft/codebert-base',
            num_labels=2,  # Binary classification
            problem_type="single_label_classification"
        )

        self.model.to(self.device)

        print(f"âœ… Model initialized!")
        print(f"   Model: microsoft/codebert-base")
        print(f"   Parameters: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"   Trainable: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}")

    def create_dataloaders(self, batch_size=8):
        """Create PyTorch DataLoaders"""
        print(f"\nğŸ“¦ Creating DataLoaders (batch_size={batch_size})...")

        train_dataset = DefectDataset(
            self.train_texts,
            self.train_labels,
            self.tokenizer
        )

        test_dataset = DefectDataset(
            self.test_texts,
            self.test_labels,
            self.tokenizer
        )

        self.train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True
        )

        self.test_loader = DataLoader(
            test_dataset,
            batch_size=batch_size,
            shuffle=False
        )

        print(f"âœ… DataLoaders created!")
        print(f"   Train batches: {len(self.train_loader)}")
        print(f"   Test batches: {len(self.test_loader)}")

    def train_epoch(self, optimizer, scheduler):
        """Train for one epoch"""
        self.model.train()
        total_loss = 0

        progress_bar = tqdm(self.train_loader, desc="Training")

        for batch in progress_bar:
            # Move to device
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)

            # Forward pass
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )

            loss = outputs.loss
            total_loss += loss.item()

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            scheduler.step()

            # Update progress bar
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

        avg_loss = total_loss / len(self.train_loader)
        return avg_loss

    def evaluate(self):
        """Evaluate model on test set"""
        self.model.eval()
        predictions = []
        true_labels = []
        probs = []

        print("\nğŸ“Š Evaluating on test set...")

        with torch.no_grad():
            for batch in tqdm(self.test_loader, desc="Evaluating"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)

                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )

                logits = outputs.logits
                preds = torch.argmax(logits, dim=1)
                probs_batch = torch.softmax(logits, dim=1)[:, 1]

                predictions.extend(preds.cpu().numpy())
                true_labels.extend(labels.cpu().numpy())
                probs.extend(probs_batch.cpu().numpy())

        # Calculate metrics
        accuracy = accuracy_score(true_labels, predictions)
        precision = precision_score(true_labels, predictions, zero_division=0)
        recall = recall_score(true_labels, predictions, zero_division=0)
        f1 = f1_score(true_labels, predictions, zero_division=0)
        auc_roc = roc_auc_score(true_labels, probs)

        results = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'auc_roc': auc_roc,
            'predictions': predictions,
            'true_labels': true_labels,
            'probs': probs
        }

        return results

    def train(self, epochs=3, batch_size=8, learning_rate=2e-5):
        """Main training loop"""
        print(f"\n{'='*80}")
        print(f"ğŸ“ STARTING TRAINING")
        print(f"{'='*80}")
        print(f"Epochs: {epochs}")
        print(f"Batch size: {batch_size}")
        print(f"Learning rate: {learning_rate}")
        print(f"{'='*80}\n")

        # Create dataloaders
        self.create_dataloaders(batch_size)

        # Setup optimizer
        optimizer = AdamW(self.model.parameters(), lr=learning_rate)

        # Setup scheduler
        total_steps = len(self.train_loader) * epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=0,
            num_training_steps=total_steps
        )

        # Training loop
        best_f1 = 0

        for epoch in range(epochs):
            print(f"\n{'='*80}")
            print(f"EPOCH {epoch + 1}/{epochs}")
            print(f"{'='*80}")

            # Train
            avg_loss = self.train_epoch(optimizer, scheduler)
            self.train_losses.append(avg_loss)

            print(f"\nâœ… Epoch {epoch + 1} completed!")
            print(f"   Average loss: {avg_loss:.4f}")

            # Evaluate
            results = self.evaluate()
            self.val_metrics.append(results)

            print(f"\nğŸ“Š Epoch {epoch + 1} Results:")
            print(f"   Accuracy:  {results['accuracy']:.4f}")
            print(f"   Precision: {results['precision']:.4f}")
            print(f"   Recall:    {results['recall']:.4f}")
            print(f"   F1-Score:  {results['f1_score']:.4f}")
            print(f"   AUC-ROC:   {results['auc_roc']:.4f}")

            # Save best model
            if results['f1_score'] > best_f1:
                best_f1 = results['f1_score']
                self.save_model()
                print(f"\nğŸ’¾ New best F1-Score! Model saved.")

        print(f"\n{'='*80}")
        print(f"ğŸ‰ TRAINING COMPLETED!")
        print(f"{'='*80}")
        print(f"Best F1-Score: {best_f1:.4f}")

        return self.val_metrics[-1]

    def save_model(self):
        """Save trained model"""
        output_dir = f'models/codebert_{self.dataset_name}'
        os.makedirs(output_dir, exist_ok=True)

        self.model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)

    def print_final_report(self, results):
        """Print detailed final report"""
        print(f"\n{'='*80}")
        print(f"ğŸ“ˆ FINAL EVALUATION REPORT - {self.dataset_name.upper()}")
        print(f"{'='*80}")

        print(f"\nğŸ“Š PERFORMANCE METRICS:")
        print(f"{'='*60}")
        print(f"{'Metric':<20} {'Value':<15}")
        print(f"{'='*60}")
        print(f"{'Accuracy':<20} {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)")
        print(f"{'Precision':<20} {results['precision']:.4f} ({results['precision']*100:.2f}%)")
        print(f"{'Recall':<20} {results['recall']:.4f} ({results['recall']*100:.2f}%)")
        print(f"{'F1-Score':<20} {results['f1_score']:.4f} ({results['f1_score']*100:.2f}%)")
        print(f"{'AUC-ROC':<20} {results['auc_roc']:.4f} ({results['auc_roc']*100:.2f}%)")
        print(f"{'='*60}")

        # Confusion Matrix
        cm = confusion_matrix(results['true_labels'], results['predictions'])
        print(f"\nğŸ“‹ CONFUSION MATRIX:")
        print(f"{'='*60}")
        print(f"                Predicted")
        print(f"              Non-Defect  Defect")
        print(f"Actual Non-D    {cm[0,0]:>6}     {cm[0,1]:>6}")
        print(f"       Defect   {cm[1,0]:>6}     {cm[1,1]:>6}")
        print(f"{'='*60}")

        # Classification Report
        print(f"\nğŸ“„ CLASSIFICATION REPORT:")
        print(classification_report(
            results['true_labels'],
            results['predictions'],
            target_names=['Non-Defective', 'Defective']
        ))

def main(dataset_name='cm1', epochs=3, batch_size=8):
    """Main training function"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘        CODEBERT FINE-TUNING FOR DEFECT PREDICTION            â•‘
    â•‘                                                              â•‘
    â•‘  Transformer-based Deep Learning Model                       â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # Initialize trainer
    trainer = CodeBERTTrainer(dataset_name=dataset_name)

    # Load data
    trainer.load_data()

    # Initialize model
    trainer.initialize_model()

    # Train
    results = trainer.train(epochs=epochs, batch_size=batch_size)

    # Print report
    trainer.print_final_report(results)

    # Save results
    results_df = pd.DataFrame([{
        'Model': 'CodeBERT',
        'Dataset': dataset_name.upper(),
        'Accuracy': f"{results['accuracy']:.4f}",
        'Precision': f"{results['precision']:.4f}",
        'Recall': f"{results['recall']:.4f}",
        'F1-Score': f"{results['f1_score']:.4f}",
        'AUC-ROC': f"{results['auc_roc']:.4f}"
    }])

    os.makedirs('results', exist_ok=True)
    results_file = f'results/{dataset_name}_codebert_results.csv'
    results_df.to_csv(results_file, index=False)
    print(f"\nğŸ’¾ Results saved: {results_file}")

    return results

if __name__ == "__main__":
    # Train on CM1 dataset (smallest, fastest for testing)
    main(dataset_name='cm1', epochs=3, batch_size=8)