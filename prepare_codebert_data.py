# -*- coding: utf-8 -*-
"""prepare_codebert_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1usn-WNjtrTae2Q7b5W1qqwYHSkkARA6Y
"""

"""
Data Preparation for CodeBERT Model
====================================
This script converts numeric features into code-like text representations
that CodeBERT can process for defect prediction.

Author: Shouvik Banik
Date: December 2025
"""

import os
import pandas as pd
import numpy as np
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

def features_to_code_text(features_dict):
    """
    Convert numeric features to a code-like text representation
    that simulates a code module with complexity metrics
    """

    # Extract key metrics
    loc = int(features_dict.get('loc', 0))
    v_g = int(features_dict.get('v(g)', 0))
    ev_g = int(features_dict.get('ev(g)', 0))
    iv_g = int(features_dict.get('iv(g)', 0))
    n = int(features_dict.get('n', 0))
    v = int(features_dict.get('v', 0))
    l = features_dict.get('l', 0)
    d = features_dict.get('d', 0)
    i = features_dict.get('i', 0)
    e = features_dict.get('e', 0)
    b = features_dict.get('b', 0)
    t = features_dict.get('t', 0)

    # Generate synthetic code-like text based on metrics
    code_text = f"""
class SoftwareModule:
    \"\"\"
    Module with {loc} lines of code
    Cyclomatic Complexity: {v_g}
    Essential Complexity: {ev_g}
    Design Complexity: {iv_g}
    \"\"\"

    def __init__(self):
        self.total_operators = {n}
        self.total_operands = {v}
        self.program_length = {n + v}
        self.vocabulary_size = {int(features_dict.get('n', 0)) + int(features_dict.get('v', 0))}
        self.difficulty = {d:.2f}
        self.effort = {e:.2f}
        self.time = {t:.2f}
        self.bugs = {b:.2f}

    def process(self):
        \"\"\"Main processing logic\"\"\"
        # Control flow paths: {v_g}
        # Essential decision points: {ev_g}
        # Module design complexity: {iv_g}

        result = None
        {"for i in range(" + str(max(1, v_g)) + "):" if v_g > 1 else ""}
        {"    " if v_g > 1 else ""}{"if " + str(i) + " < " + str(max(1, ev_g)) + ":" if ev_g > 0 else "pass"}
        {"        " if v_g > 1 and ev_g > 0 else ""}{"result = self.compute()" if ev_g > 0 else ""}

        return result

    def compute(self):
        \"\"\"Computation method\"\"\"
        # Lines of code: {loc}
        # Halstead volume: {v:.2f}
        value = 0
        {"for j in range(" + str(min(10, loc // 5)) + "):" if loc > 5 else ""}
        {"    " if loc > 5 else ""}value += {"j" if loc > 5 else "1"}
        return value
"""

    return code_text.strip()

def prepare_codebert_dataset(input_file, output_file):
    """
    Convert processed numeric dataset to CodeBERT-compatible text format
    """
    print(f"\n{'='*80}")
    print(f"Processing: {input_file}")
    print(f"{'='*80}")

    # Load data
    df = pd.read_csv(input_file)
    print(f"âœ… Loaded {len(df)} samples")

    # Prepare lists for code text and labels
    code_texts = []
    labels = []

    print("\nğŸ”„ Converting features to code-like text...")
    for idx, row in tqdm(df.iterrows(), total=len(df)):
        # Extract features (all columns except 'target')
        features = row.drop('target').to_dict()

        # Convert to code text
        code_text = features_to_code_text(features)
        code_texts.append(code_text)

        # Get label
        labels.append(int(row['target']))

    # Create new dataframe
    codebert_df = pd.DataFrame({
        'code': code_texts,
        'label': labels
    })

    # Save
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    codebert_df.to_csv(output_file, index=False)

    print(f"\nâœ… Saved CodeBERT dataset: {output_file}")
    print(f"   Total samples: {len(codebert_df)}")
    print(f"   Defective: {sum(labels)} ({sum(labels)/len(labels)*100:.1f}%)")
    print(f"   Non-defective: {len(labels) - sum(labels)} ({(len(labels)-sum(labels))/len(labels)*100:.1f}%)")

    # Show sample
    print(f"\nğŸ“„ Sample code text (first 500 chars):")
    print("="*80)
    print(codebert_df['code'].iloc[0][:500] + "...")
    print("="*80)

    return codebert_df

def process_all_datasets():
    """Process all train/test datasets"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘        PREPARING DATA FOR CODEBERT MODEL                     â•‘
    â•‘                                                              â•‘
    â•‘  Converting numeric features to code-like text               â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    datasets = ['cm1', 'pc1', 'jm1', 'kc1']

    for dataset in datasets:
        train_input = f'data/processed/{dataset}_train.csv'
        test_input = f'data/processed/{dataset}_test.csv'

        train_output = f'data/codebert/{dataset}_train_code.csv'
        test_output = f'data/codebert/{dataset}_test_code.csv'

        if os.path.exists(train_input):
            print(f"\n{'#'*80}")
            print(f"# DATASET: {dataset.upper()}")
            print(f"{'#'*80}")

            # Process train
            prepare_codebert_dataset(train_input, train_output)

            # Process test
            prepare_codebert_dataset(test_input, test_output)

            print(f"\nâœ… {dataset.upper()} complete!")
        else:
            print(f"\nâš ï¸ {dataset.upper()} not found, skipping...")

    print(f"\n{'#'*80}")
    print(f"# âœ… ALL DATASETS PREPARED FOR CODEBERT!")
    print(f"# ğŸ“ Output directory: data/codebert/")
    print(f"{'#'*80}\n")

def verify_output():
    """Verify the created datasets"""
    print("\nğŸ“Š VERIFICATION SUMMARY:")
    print("="*80)

    datasets = ['cm1', 'pc1', 'jm1', 'kc1']

    for dataset in datasets:
        train_file = f'data/codebert/{dataset}_train_code.csv'
        test_file = f'data/codebert/{dataset}_test_code.csv'

        if os.path.exists(train_file) and os.path.exists(test_file):
            train_df = pd.read_csv(train_file)
            test_df = pd.read_csv(test_file)

            print(f"\n{dataset.upper()}:")
            print(f"  Train: {len(train_df)} samples")
            print(f"  Test:  {len(test_df)} samples")
            print(f"  Avg code length: {train_df['code'].str.len().mean():.0f} chars")
        else:
            print(f"\n{dataset.upper()}: âš ï¸ Files not found")

    print("\n" + "="*80)

def main():
    """Main execution"""
    # Process all datasets
    process_all_datasets()

    # Verify output
    verify_output()

    print("\nâœ… Data preparation complete!")
    print("\nğŸ“ NEXT STEPS:")
    print("   1. Review the generated code samples in data/codebert/")
    print("   2. Run the CodeBERT training script")
    print("   3. Compare results with baseline models")

if __name__ == "__main__":
    main()